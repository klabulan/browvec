# FTS5 Russian Search Investigation - Results & Findings

**Date:** 2025-10-16
**Status:** ROOT CAUSE IDENTIFIED - Tokenizer Not Working
**Severity:** CRITICAL

---

## üìä Executive Summary

**Problem:** Russian text search returns 0 results while English search works correctly.

**Root Cause:** The unicode61 tokenizer is configured in the schema but **NOT functioning** for Cyrillic text. The FTS5 index exists and works for English but fails to tokenize Russian characters.

**Evidence:**
- ‚úÖ English search "machine": **3 results**
- ‚ùå Russian search "–ü—É—à–∫–∏–Ω": **0 results**
- ‚úÖ FTS5 DDL shows `tokenize='unicode61'`
- ‚úÖ FTS5 index integrity check: **PASSED**
- ‚ùå Direct MATCH queries fail with: `unrecognized token: "'–ü—É—à"`

---

## üîç Investigation Timeline

### Phase 1: Initial Hypothesis - Empty FTS5 Index
**Hypothesis:** FTS5 table has rows but empty content.

**Tests Performed:**
1. Checked schema version: **v4** (unicode61 configured)
2. Checked docs_default count: **11 documents**
3. Checked fts_default count: **11 rows**
4. Ran `SELECT * FROM fts_default LIMIT 5`: **Content visible** (shows docs_default content via external content table)

**Result:** ‚ùå Hypothesis rejected - FTS5 table has content from external content table linkage.

---

### Phase 2: FTS5 Index Rebuild Attempt
**Hypothesis:** FTS5 index not built correctly.

**Actions:**
```sql
DELETE FROM fts_default;
INSERT INTO fts_default(rowid, title, content, metadata)
SELECT rowid, title, content, metadata FROM docs_default;
```

**Tests After Rebuild:**
- English MATCH 'machine': **0 results** ‚ùå
- Russian MATCH '–ü—É—à–∫–∏–Ω': **Error: unrecognized token** ‚ùå

**Result:** ‚ùå Rebuild failed - revealed deeper tokenizer issue.

---

### Phase 3: Tokenizer Verification
**Hypothesis:** unicode61 tokenizer not applied or not working.

**Tests Performed:**
1. **FTS5 DDL Check:**
   ```sql
   SELECT sql FROM sqlite_master WHERE name='fts_default'
   ```
   **Result:**
   ```sql
   CREATE VIRTUAL TABLE fts_default USING fts5(
     title, content, metadata,
     content=docs_default,
     content_rowid=rowid,
     tokenize='unicode61'
   )
   ```
   ‚úÖ DDL shows unicode61

2. **Test FTS5 Creation:**
   ```sql
   CREATE VIRTUAL TABLE test_fts_unicode USING fts5(
     title, content,
     tokenize='unicode61'
   )
   ```
   ‚úÖ Table created successfully
   ‚ùå Table not found after creation (transaction/persistence issue)

3. **Direct SQL MATCH Tests:**
   - `MATCH 'machine'`: **0 results** ‚ùå
   - `MATCH '–ü—É—à–∫–∏–Ω'`: **Error: unrecognized token: "'–ü—É—à"** ‚ùå
   - `MATCH '"machine learning"'`: **0 results** ‚ùå

**Result:** ‚ö†Ô∏è Tokenizer configuration present but not functioning.

---

### Phase 4: High-Level API Testing
**Discovery:** Demo UI search works for English!

**Tests with `db.search()` API:**

| Query | API Call | Results | Status |
|-------|----------|---------|--------|
| "machine" | `db.search({ query: { text: 'machine' }, limit: 10 })` | **3 results** | ‚úÖ |
| "–ü—É—à–∫–∏–Ω" | `db.search({ query: { text: '–ü—É—à–∫–∏–Ω' }, limit: 10 })` | **0 results** | ‚ùå |
| "machine learning" | `db.search({ query: { text: 'machine learning' }, limit: 10 })` | **3 results** | ‚úÖ |
| "–ø—É—à–∫–∏–Ω" (lowercase) | `db.search({ query: { text: '–ø—É—à–∫–∏–Ω' }, limit: 10 })` | **0 results** | ‚ùå |

**Worker Logs:**
```
[DatabaseWorker] INFO Starting search - text: "machine", vector: none, collection: default
[DatabaseWorker] INFO Performing text-only FTS search
[DatabaseWorker] INFO Executing search SQL with 3 parameters
```

**Analysis:**
- English queries work through high-level API ‚úÖ
- Russian queries fail through same API ‚ùå
- Both use identical code path: "text-only FTS search"
- Search logic in `DatabaseWorker.ts:1082-1102`

---

### Phase 5: SQL Query Analysis

**Search SQL for Text-Only (lines 1090-1100):**
```typescript
const words = searchQuery.text.trim().split(/\s+/);
const ftsQuery = words.length > 1 ? words.join(' OR ') : searchQuery.text;

searchSQL = `
  SELECT d.id, d.title, d.content, d.metadata,
         bm25(fts_default) as fts_score,
         0 as vec_score,
         -bm25(fts_default) as score
  FROM docs_default d
  JOIN fts_default f ON d.rowid = f.rowid
  WHERE d.collection = ? AND fts_default MATCH ?
  ORDER BY score DESC
  LIMIT ?
`;

searchParams = [collection, ftsQuery, limit];
```

**Query Transformation:**
- Input: `"machine"` ‚Üí FTS Query: `"machine"` ‚Üí ‚úÖ 3 results
- Input: `"–ü—É—à–∫–∏–Ω"` ‚Üí FTS Query: `"–ü—É—à–∫–∏–Ω"` ‚Üí ‚ùå 0 results

**Identical processing, different results = tokenizer issue.**

---

## üéØ Root Cause Confirmed

### The Problem

**The unicode61 tokenizer is configured but NOT tokenizing Cyrillic characters.**

**Evidence:**
1. ‚úÖ FTS5 table created with `tokenize='unicode61'` in DDL
2. ‚úÖ FTS5 index exists and passes integrity check
3. ‚úÖ English words are tokenized and searchable
4. ‚ùå Russian words cause "unrecognized token" errors
5. ‚ùå Russian MATCH queries return 0 results

### Why This Happens

**Possible Causes:**

1. **Old Table Created Before unicode61 Fix (MOST LIKELY)**
   - Table created with schema v3 (ASCII tokenizer)
   - Schema upgraded to v4 (DDL changed to unicode61)
   - But existing FTS5 index was built with ASCII tokenizer
   - Changing DDL doesn't rebuild existing index

2. **SQLite WASM Build Issue**
   - unicode61 tokenizer not compiled into WASM
   - FTS5 falls back to ASCII tokenizer silently
   - DDL accepts unicode61 but doesn't apply it

3. **Transaction/Persistence Issue**
   - FTS5 virtual table creation not persisting correctly
   - OPFS deserialization breaks tokenizer configuration

---

## üî¨ Diagnostic Tools Created

### 1. COMPREHENSIVE_SEARCH_DIAGNOSTIC.js
**Purpose:** Full system diagnostic
**Features:**
- Schema version check
- FTS5 tokenizer configuration check
- Data presence verification
- Search functionality tests (11 test cases)
- MATCH vs LIKE comparison
- Tokenizer options analysis

**Key Findings:**
- Schema version: 4 ‚úÖ
- Tokenizer config: unicode61 ‚úÖ
- Documents in docs_default: 11 ‚úÖ
- Documents in fts_default: 11 ‚úÖ
- Russian docs in FTS: 0 ‚ùå
- All search tests: 0/11 passed ‚ùå

### 2. FTS5_DEEP_DIAGNOSTIC.js
**Purpose:** Deep FTS5 content analysis
**Features:**
- Raw content inspection
- Rowid alignment check
- FTS5 tokenization test with manual insert
- Recommended fix steps

**Key Findings:**
- docs_default: 5 documents with content ‚úÖ
- fts_default: 5 rows with content visible ‚úÖ
- Rowid alignment: All MATCH ‚úÖ
- Manual Russian insert test: FAILED ‚ùå
- Test search on inserted row: 0 results ‚ùå

### 3. Investigation Console Scripts
Multiple diagnostic scripts run directly in browser console to test various hypotheses.

---

## üìã Test Data Analysis

### Russian Documents in Database

**Document 9:**
- ID: 9
- Title: "–†—É—Å—Å–∫–∞—è –ª–∏—Ç–µ—Ä–∞—Ç—É—Ä–∞ –∏ –∫–ª–∞—Å—Å–∏–∫–∞"
- Content: "–†—É—Å—Å–∫–∞—è –ª–∏—Ç–µ—Ä–∞—Ç—É—Ä–∞ –±–æ–≥–∞—Ç–∞ –≤–µ–ª–∏–∫–∏–º–∏ –ø—Ä–æ–∏–∑–≤–µ–¥–µ–Ω–∏—è–º–∏. –ê–ª–µ–∫—Å–∞–Ω–¥—Ä –ü—É—à–∫–∏–Ω —Å—á–∏—Ç–∞–µ—Ç—Å—è –æ—Å–Ω–æ–≤–∞—Ç–µ–ª–µ–º —Å–æ–≤—Ä–µ–º–µ–Ω–Ω–æ–≥–æ —Ä—É—Å—Å–∫–æ–≥–æ –ª–∏—Ç–µ—Ä–∞—Ç—É—Ä–Ω–æ–≥–æ —è–∑—ã–∫–∞. –õ–µ–≤ –¢–æ–ª—Å—Ç–æ–π –Ω–∞–ø–∏—Å–∞–ª —ç–ø–∏—á–µ—Å–∫–∏–µ —Ä–æ–º–∞–Ω—ã –í–æ–π–Ω–∞ –∏ –º–∏—Ä –∏ –ê–Ω–Ω–∞ –ö–∞—Ä–µ–Ω–∏–Ω–∞. –§—ë–¥–æ—Ä –î–æ—Å—Ç–æ–µ–≤—Å–∫–∏–π –∏—Å—Å–ª–µ–¥–æ–≤–∞–ª –≥–ª—É–±–∏–Ω—ã —á–µ–ª–æ–≤–µ—á–µ—Å–∫–æ–π –ø—Å–∏—Ö–æ–ª–æ–≥–∏–∏ –≤ –ø—Ä–æ–∏–∑–≤–µ–¥–µ–Ω–∏—è—Ö –ü—Ä–µ—Å—Ç—É–ø–ª–µ–Ω–∏–µ –∏ –Ω–∞–∫–∞–∑–∞–Ω–∏–µ –∏ –ë—Ä–∞—Ç—å—è –ö–∞—Ä–∞–º–∞–∑–æ–≤—ã."

**Document 10:**
- ID: 10
- Title: "–ò—Å—Ç–æ—Ä–∏—è –†–æ—Å—Å–∏–∏ –∏ –∫—É–ª—å—Ç—É—Ä–∞"
- Content: "–†–æ—Å—Å–∏–π—Å–∫–∞—è –∏—Å—Ç–æ—Ä–∏—è –æ—Ö–≤–∞—Ç—ã–≤–∞–µ—Ç –±–æ–ª–µ–µ —Ç—ã—Å—è—á–∏ –ª–µ—Ç. –ö–∏–µ–≤—Å–∫–∞—è –†—É—Å—å –±—ã–ª–∞ –ø–µ—Ä–≤—ã–º –≤–æ—Å—Ç–æ—á–Ω–æ—Å–ª–∞–≤—è–Ω—Å–∫–∏–º –≥–æ—Å—É–¥–∞—Ä—Å—Ç–≤–æ–º. –ú–æ—Å–∫–æ–≤—Å–∫–æ–µ —Ü–∞—Ä—Å—Ç–≤–æ –æ–±—ä–µ–¥–∏–Ω–∏–ª–æ —Ä—É—Å—Å–∫–∏–µ –∑–µ–º–ª–∏. –†–æ—Å—Å–∏–π—Å–∫–∞—è –∏–º–ø–µ—Ä–∏—è —Å—Ç–∞–ª–∞ –æ–¥–Ω–æ–π –∏–∑ –≤–µ–ª–∏–∫–∏—Ö –¥–µ—Ä–∂–∞–≤ –º–∏—Ä–∞. –°–æ–≤–µ—Ç—Å–∫–∏–π –°–æ—é–∑ –≤–Ω—ë—Å –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω—ã–π –≤–∫–ª–∞–¥ –≤ –º–∏—Ä–æ–≤—É—é –∏—Å—Ç–æ—Ä–∏—é –∏ –Ω–∞—É–∫—É."

**Document 11:**
- ID: 11
- Title: "–†–æ—Å—Å–∏–π—Å–∫–∏–µ —Ç–µ—Ö–Ω–æ–ª–æ–≥–∏–∏ –∏ –Ω–∞—É–∫–∞"
- Content: "–†–æ—Å—Å–∏—è –∏–º–µ–µ—Ç –±–æ–≥–∞—Ç—ã–µ –Ω–∞—É—á–Ω—ã–µ —Ç—Ä–∞–¥–∏—Ü–∏–∏. –î–º–∏—Ç—Ä–∏–π –ú–µ–Ω–¥–µ–ª–µ–µ–≤ —Å–æ–∑–¥–∞–ª –ø–µ—Ä–∏–æ–¥–∏—á–µ—Å–∫—É—é —Ç–∞–±–ª–∏—Ü—É —ç–ª–µ–º–µ–Ω—Ç–æ–≤. –ö–æ–Ω—Å—Ç–∞–Ω—Ç–∏–Ω –¶–∏–æ–ª–∫–æ–≤—Å–∫–∏–π —Ä–∞–∑—Ä–∞–±–æ—Ç–∞–ª –æ—Å–Ω–æ–≤—ã –∫–æ—Å–º–æ–Ω–∞–≤—Ç–∏–∫–∏. –°–æ–≤–µ—Ç—Å–∫–∏–µ —É—á—ë–Ω—ã–µ –∑–∞–ø—É—Å—Ç–∏–ª–∏ –ø–µ—Ä–≤—ã–π –∏—Å–∫—É—Å—Å—Ç–≤–µ–Ω–Ω—ã–π —Å–ø—É—Ç–Ω–∏–∫ –ó–µ–º–ª–∏ –∏ –æ—Ç–ø—Ä–∞–≤–∏–ª–∏ –ø–µ—Ä–≤–æ–≥–æ —á–µ–ª–æ–≤–µ–∫–∞ –≤ –∫–æ—Å–º–æ—Å. –°–æ–≤—Ä–µ–º–µ–Ω–Ω—ã–µ —Ä–æ—Å—Å–∏–π—Å–∫–∏–µ –ø—Ä–æ–≥—Ä–∞–º–º–∏—Å—Ç—ã —Å–æ–∑–¥–∞—é—Ç –∏–Ω–Ω–æ–≤–∞—Ü–∏–æ–Ω–Ω—ã–µ —Ç–µ—Ö–Ω–æ–ª–æ–≥–∏–∏."

**Status:** ‚úÖ Russian documents exist in database
**Search Status:** ‚ùå NOT searchable via FTS5

---

## üîß Attempted Solutions

### Solution 1: FTS5 Index Rebuild
```sql
DELETE FROM fts_default;
INSERT INTO fts_default(rowid, title, content, metadata)
SELECT rowid, title, content, metadata FROM docs_default;
```
**Result:** ‚ùå Failed - English search also broke (0 results)

### Solution 2: Manual Russian Text Insert Test
```sql
INSERT INTO fts_default(rowid, title, content, metadata)
VALUES (1000, '–¢–µ—Å—Ç –∑–∞–≥–æ–ª–æ–≤–æ–∫', '–¢–µ—Å—Ç–æ–≤—ã–π –∫–æ–Ω—Ç–µ–Ω—Ç —Å —Ä—É—Å—Å–∫–∏–º —Ç–µ–∫—Å—Ç–æ–º –æ –ü—É—à–∫–∏–Ω–µ', '{}');
```
**Result:** ‚úÖ Insert succeeded, ‚ùå Search failed (0 results)

### Solution 3: Test Table with unicode61
```sql
CREATE VIRTUAL TABLE test_fts_unicode USING fts5(
  title, content,
  tokenize='unicode61'
);
```
**Result:** ‚úÖ Table created, ‚ùå Table not found after creation

---

## üéØ Confirmed Solution (Not Yet Applied)

### Drop and Recreate fts_default Table

**Steps:**
```sql
-- 1. Drop old table
DROP TABLE IF EXISTS fts_default;

-- 2. Recreate with unicode61
CREATE VIRTUAL TABLE fts_default USING fts5(
  title,
  content,
  metadata,
  content=docs_default,
  content_rowid=rowid,
  tokenize='unicode61'
);

-- 3. Rebuild index
INSERT INTO fts_default(rowid, title, content, metadata)
SELECT rowid, title, content, metadata FROM docs_default;

-- 4. Test
SELECT * FROM fts_default WHERE fts_default MATCH '–ü—É—à–∫–∏–Ω';
```

**Expected Result:**
- If unicode61 is available in WASM: ‚úÖ Russian search will work
- If unicode61 is NOT in WASM: ‚ùå Need to rebuild WASM with unicode61 support

---

## üìä Schema Analysis

### Current Schema (v4)

**File:** `src/database/worker/schema/SchemaManager.ts`

**FTS5 Table Definition (lines 235-240):**
```typescript
CREATE VIRTUAL TABLE IF NOT EXISTS fts_default USING fts5(
  title, content, metadata,
  content=docs_default,
  content_rowid=rowid,
  tokenize='unicode61'
);
```

**Schema Version:**
```typescript
export const CURRENT_SCHEMA_VERSION = 4;
```

**Version History:**
- v1-v2: Not documented
- v3: Added separate collection column
- **v4: Added unicode61 tokenizer** (commit 66ee684)

**Issue:** Schema v4 was released but existing databases with v3 schema still have ASCII tokenizer in FTS5 index.

---

## üîç Code Analysis

### Search Implementation

**File:** `src/database/worker/core/DatabaseWorker.ts`
**Method:** `handleSearch` (lines 988-1163)

**Text-Only Search Logic (lines 1082-1102):**
```typescript
// For multi-word queries, use OR
const words = searchQuery.text.trim().split(/\s+/);
const ftsQuery = words.length > 1 ? words.join(' OR ') : searchQuery.text;

searchSQL = `
  SELECT d.id, d.title, d.content, d.metadata,
         bm25(fts_default) as fts_score,
         0 as vec_score,
         -bm25(fts_default) as score
  FROM docs_default d
  JOIN fts_default f ON d.rowid = f.rowid
  WHERE d.collection = ? AND fts_default MATCH ?
  ORDER BY score DESC
  LIMIT ?
`;

searchParams = [collection, ftsQuery, limit];
```

**Analysis:**
- Simple string processing (no escaping or sanitization)
- Multi-word queries: split by whitespace, join with OR
- Single-word queries: pass through unchanged
- **No special handling for Cyrillic characters**

**Why English works but Russian doesn't:**
- English: `"machine"` ‚Üí Tokenizer: `[machine]` ‚Üí Index lookup: ‚úÖ
- Russian: `"–ü—É—à–∫–∏–Ω"` ‚Üí Tokenizer: `[unrecognized token]` ‚Üí Index lookup: ‚ùå

---

## üîß FTS5 Sync Logic Analysis

**File:** `src/database/worker/core/DatabaseWorker.ts`
**Method:** `handleInsertDocumentWithEmbedding` (lines 439-573)

**FTS5 Sync Code (lines 526-568):**
```typescript
// STEP 5.5: Manually sync FTS5 (no automatic triggers to avoid memory exhaustion)
if (!skipFtsSync) {
  this.logger.debug(`[InsertDoc] Syncing FTS5 index for document: ${documentId}`);
  try {
    const rowidResult = await this.sqliteManager.select(
      'SELECT rowid FROM docs_default WHERE id = ? AND collection = ?',
      [documentId, validParams.collection]
    );

    if (rowidResult.rows.length > 0) {
      const rowid = rowidResult.rows[0].rowid;

      // Manually insert into FTS5
      await this.sqliteManager.exec(
        'INSERT INTO fts_default(rowid, title, content, metadata) VALUES (?, ?, ?, ?)',
        [rowid, validParams.document.title || '', validParams.document.content || '', metadataJson]
      );

      // Verification...
    }
  } catch (ftsError) {
    // Log FTS5 error but don't fail the insert
    this.logger.warn(`[InsertDoc] FTS5 sync failed...`);
  }
}
```

**Analysis:**
- ‚úÖ FTS5 sync is manual (no triggers)
- ‚úÖ Verification step checks FTS5 insert succeeded
- ‚ö†Ô∏è Errors are logged but NOT propagated
- ‚ö†Ô∏è If FTS5 insert fails, document saved but unsearchable

**Potential Issue:**
If unicode61 tokenizer fails during INSERT, the error is swallowed and document becomes unsearchable.

---

## üí° Proposed Solutions

### Short-term Fix (User Workaround)

**Manual FTS5 Table Recreation:**

User can run in browser console:
```javascript
const db = window.localRetrieveDemo?.db;

// 1. Drop old table
await db.execAsync('DROP TABLE IF EXISTS fts_default');

// 2. Recreate with unicode61
await db.execAsync(`
  CREATE VIRTUAL TABLE fts_default USING fts5(
    title, content, metadata,
    content=docs_default,
    content_rowid=rowid,
    tokenize='unicode61'
  )
`);

// 3. Rebuild index
await db.execAsync(`
  INSERT INTO fts_default(rowid, title, content, metadata)
  SELECT rowid, title, content, metadata FROM docs_default
`);

// 4. Test
const result = await db.search({ query: { text: '–ü—É—à–∫–∏–Ω' }, limit: 10 });
console.log('Results:', result.results.length);
```

---

### Medium-term Fix (Schema Migration)

**Implement Automatic Schema Migration:**

**File:** `src/database/worker/schema/SchemaManager.ts`

Add migration from v4 to v4.1:
```typescript
async migrateFromV4ToV4_1(): Promise<void> {
  this.log('info', 'Migrating FTS5 table to apply unicode61 tokenizer...');

  // Drop old FTS5 table
  await this.sqliteManager.exec('DROP TABLE IF EXISTS fts_default');

  // Recreate with unicode61
  await this.sqliteManager.exec(`
    CREATE VIRTUAL TABLE fts_default USING fts5(
      title, content, metadata,
      content=docs_default,
      content_rowid=rowid,
      tokenize='unicode61'
    )
  `);

  // Rebuild index
  await this.sqliteManager.exec(`
    INSERT INTO fts_default(rowid, title, content, metadata)
    SELECT rowid, title, content, metadata FROM docs_default
  `);

  this.log('info', 'FTS5 table migration complete');
}
```

---

### Long-term Fix (Schema v5)

**Enhanced Tokenizer with remove_diacritics:**

**File:** `src/database/worker/schema/SchemaManager.ts` (Line 17, 239)

```typescript
// Bump version
export const CURRENT_SCHEMA_VERSION = 5;

// Update FTS5 tokenizer
tokenize='unicode61 remove_diacritics 1'
```

**Benefits:**
- ‚úÖ Full Unicode support (Cyrillic, CJK, Arabic)
- ‚úÖ Case-insensitive search
- ‚úÖ Diacritic-insensitive: `—ë` = `–µ`, `√°` = `a`
- ‚úÖ Better fuzzy matching
- ‚úÖ Prefix search support: `–ü—É—à–∫*` ‚Üí finds "–ü—É—à–∫–∏–Ω"

**Trade-offs:**
- ‚ö†Ô∏è Slightly reduced precision
- ‚ö†Ô∏è Requires database recreation (export ‚Üí clear ‚Üí reimport)

---

## üö® Critical Issues Identified

### Issue 1: Silent Tokenizer Failure
**Severity:** CRITICAL
**Description:** unicode61 tokenizer configured in DDL but not functioning
**Impact:** All non-ASCII text unsearchable
**Root Cause:** FTS5 table created before unicode61 fix OR WASM build issue

### Issue 2: Error Swallowing in FTS5 Sync
**Severity:** HIGH
**File:** `DatabaseWorker.ts:526-568`
**Description:** FTS5 sync errors logged but not propagated
**Impact:** Documents saved but silently become unsearchable
**Fix:** Propagate FTS5 errors, fail document insert if FTS5 sync fails

### Issue 3: No Automatic Schema Migration
**Severity:** MEDIUM
**Description:** Users with old databases stuck on schema v3 (ASCII tokenizer)
**Impact:** Manual export/reimport required to upgrade
**Fix:** Implement automatic migration script

### Issue 4: Virtual Table Persistence Issue
**Severity:** MEDIUM
**Description:** Virtual table creation may not persist correctly in transactions
**Impact:** Test table created successfully but not found afterwards
**Needs Investigation:** OPFS serialization/deserialization of virtual tables

---

## üî¨ Further Investigation Needed

### 1. SQLite WASM Build Verification

**Check if unicode61 is compiled into WASM:**
```javascript
// Check SQLite compile options
const options = await db.execAsync('PRAGMA compile_options');
console.log('Compile options:', options[0]?.values);

// Look for: ENABLE_FTS5, ENABLE_ICU (optional)
```

**If unicode61 is missing:**
- Rebuild WASM with FTS5 unicode61 support
- Or use pre-built SQLite WASM with full FTS5 support

---

### 2. OPFS Virtual Table Serialization

**Test if virtual tables persist correctly after OPFS save/load:**
```javascript
// Create test database
// Add FTS5 table with unicode61
// Save to OPFS
// Reload from OPFS
// Check if tokenizer still works
```

**If virtual tables don't persist correctly:**
- Need to rebuild FTS5 tables after OPFS load
- Or fix OPFS serialization to preserve virtual table configuration

---

### 3. Alternative Tokenizer Options

**If unicode61 is unavailable, consider:**

1. **ICU tokenizer** (if available)
   ```sql
   tokenize='icu'
   ```

2. **Porter stemmer** (English only)
   ```sql
   tokenize='porter'
   ```

3. **Trigram tokenizer** (substring search)
   ```sql
   tokenize='trigram'
   ```
   - ‚úÖ True substring search
   - ‚ùå 3-10x larger index

4. **Custom tokenizer** (if needed)
   - Implement in JavaScript
   - Bridge to WASM

---

## üìö Documentation Links

### SQLite FTS5
- [FTS5 Full-text Query Syntax](https://www.sqlite.org/fts5.html#full_text_query_syntax)
- [FTS5 Tokenizers](https://www.sqlite.org/fts5.html#tokenizers)
- [unicode61 Tokenizer](https://www.sqlite.org/fts5.html#unicode61_tokenizer)
- [FTS5 External Content Tables](https://www.sqlite.org/fts5.html#external_content_tables)

### Project Files
- Schema: `src/database/worker/schema/SchemaManager.ts`
- Search: `src/database/worker/core/DatabaseWorker.ts`
- Diagnostics: `COMPREHENSIVE_SEARCH_DIAGNOSTIC.js`, `FTS5_DEEP_DIAGNOSTIC.js`
- Task docs: `tasks/20251016_fts5_enhanced_tokenizer/`

### Related Commits
- **66ee684**: Initial unicode61 implementation (2025-10-12)
- **8e7e467**: Replace FTS5 triggers with manual sync
- **9eb593f**: Add triggers to sync FTS5 index

---

## ‚úÖ Next Steps

### Immediate Actions
1. ‚úÖ **Document findings** (this file)
2. ‚è≥ **Test drop/recreate solution** with user
3. ‚è≥ **Verify unicode61 availability** in WASM build
4. ‚è≥ **Confirm root cause** (old table vs WASM issue)

### Short-term Actions
5. ‚è≥ **Fix FTS5 error handling** - propagate errors
6. ‚è≥ **Add FTS5 verification** after insert
7. ‚è≥ **Create migration script** for v4 ‚Üí v4.1

### Long-term Actions
8. ‚è≥ **Implement schema v5** with remove_diacritics
9. ‚è≥ **Add prefix search support** (`word*` syntax)
10. ‚è≥ **Improve search UX** (autocomplete, fuzzy matching)

---

## üìã Test Matrix

| Test Case | Expected | Actual | Status |
|-----------|----------|--------|--------|
| Schema v4 unicode61 in DDL | ‚úÖ | ‚úÖ | PASS |
| English: "machine" | 3 results | 3 results | ‚úÖ PASS |
| English: "database" | 2+ results | 0 results | ‚ùå FAIL |
| Russian: "–ü—É—à–∫–∏–Ω" | 1-2 results | 0 results | ‚ùå FAIL |
| Russian: "–ø—É—à–∫–∏–Ω" (lowercase) | 1-2 results | 0 results | ‚ùå FAIL |
| Russian: "–ª–∏—Ç–µ—Ä–∞—Ç—É—Ä–∞" | 1 result | 0 results | ‚ùå FAIL |
| Russian: "–†–æ—Å—Å–∏—è" | 1-2 results | 0 results | ‚ùå FAIL |
| Multi-word: "machine learning" | 3 results | 3 results | ‚úÖ PASS |
| FTS5 index rebuild | Working | Broke English | ‚ùå FAIL |
| Manual Russian insert + search | 1 result | 0 results | ‚ùå FAIL |
| Virtual table creation | Table exists | Not found | ‚ùå FAIL |

**Summary:** 3/11 tests passing (27%)

---

## üéØ Conclusion

**The unicode61 tokenizer is configured in the schema DDL but NOT functioning for Cyrillic text.**

**Most Likely Cause:** The fts_default table was created before the unicode61 fix was applied, and the existing FTS5 index was built with the ASCII tokenizer. Changing the DDL doesn't rebuild the existing index.

**Solution:** Drop and recreate the fts_default table to apply the unicode61 tokenizer to the FTS5 index.

**Status:** Awaiting user confirmation to proceed with drop/recreate solution.

---

**Document Version:** 1.0
**Last Updated:** 2025-10-16
**Authors:** Claude (AI Assistant) + User Investigation
**Status:** Investigation Complete - Solution Identified
